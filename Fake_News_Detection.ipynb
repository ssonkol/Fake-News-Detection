{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUsEf4iavo3u"
      },
      "source": [
        "# **Fake News**\n",
        "\n",
        "<body>\n",
        "<img src=\"https://conversationagent.typepad.com/.a/6a00d8341c03bb53ef0147e02d8fa5970b-pi\" width=\"870\"/>\n",
        "</body>\n",
        "\n",
        "**The Daily Inspector was a popular news agency which was once trusted for its reliable and speedy news coverage with over 4 million readers worldwide.**\n",
        "\n",
        "**A recent lapse in the news agency’s fact-checking policy meant that over 1,000 articles went out within the last 2 months with incorrect information; leading to large criticism and a significant drop in the agency’s number of readers.**\n",
        "\n",
        "**To rectify this, The Daily Inspector has hired you to find a way of locating these fake articles; so that they can be deleted or corrected, and suggest potential ways they can prevent such a thing from happening again.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAJ8sQB8u7lj"
      },
      "source": [
        "## **1. Import Libraries and Data needed**\n",
        "<body>\n",
        "<img src=\"https://offloadmedia.feverup.com/secretldn.com/wp-content/uploads/2016/06/18075319/Libraries-1024x901.jpg\" width=\"600\"/>\n",
        "</body>\n",
        "\n",
        "Here we are importing all the libraries that we will need to analyse the data.\n",
        "\n",
        "Libraries contain all the little functions and tools that other programmers have created. This way we don't have to spend hours recreating code. Instead, we just call out the function name and it performs all the steps we want it to do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91GhjKd9E3eu"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFGRIGmG77t1"
      },
      "source": [
        "## Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAF-B6ps7-Bu"
      },
      "source": [
        "## Import train dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ssonkol/Fake-News-Detection/main/train.csv',engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "## Import test dataset\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/ssonkol/Fake-News-Detection/main/test.csv',engine='python', encoding='utf-8', error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oetFrIdV8EXM"
      },
      "source": [
        "## **2. Exploratory Data Analysis & Text Processing**\n",
        "\n",
        "First let's inspect the data\n",
        "\n",
        "Note: \n",
        "\n",
        "1 = Fake News/Article\n",
        "\n",
        "0 = Not Fake News/Article\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFxJkmCk7-OR"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dsaXQjjxc7M"
      },
      "source": [
        "Let's check if there are any null values in the data!\n",
        "\n",
        "Models can't make any predictions and we can't make any useful insights with null values in our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWPi_eDd7-Q4"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hU1B3b0DHkw"
      },
      "source": [
        "#### **Data Prep**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h68Eo2QEDMIL"
      },
      "source": [
        "# Assign nan in place of blanks in the text column\n",
        "df['text'] = df['text'].str.strip()\n",
        "df['text'] = df['text'].replace(r'^\\s*$', np.nan, regex=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV3E8VRw7-Ta"
      },
      "source": [
        "# Remove all rows where complaints column is nan\n",
        "df.dropna(subset=['text'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MelbxNm1DbHX"
      },
      "source": [
        "####**Check for duplicates**\n",
        "\n",
        "Having duplicates in any dataset isn't good. But it is especially important to create effective and accurate models.\n",
        "\n",
        "Having data with no duplicates ensures that you will develop one complete version of the truth, allowing you to base strategic decisions on accurate data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7c0HD7XDaWY"
      },
      "source": [
        "df.duplicated(subset=[\"text\"]).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2BXigyTDgQj"
      },
      "source": [
        "As we can see, we have few duplicate entries for text column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAb_BrE9DaY5"
      },
      "source": [
        "dup = df[df.duplicated(subset=[\"text\"])]\n",
        "dup.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmsXtuOTDabZ"
      },
      "source": [
        "# print one duplicate entry\n",
        "df[df['text'] == dup.loc[480]['text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkYBcjnDhpPW"
      },
      "source": [
        "**Question**\n",
        "\n",
        "Why would you want to remove these duplicate entries?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSFX3wa3DwfG"
      },
      "source": [
        "####**Drop duplicated Data & Nan values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6kPiqJxDaf-"
      },
      "source": [
        "# drop duplicated data\n",
        "df = df.drop_duplicates(subset={\"text\"}, keep='first', inplace=False)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdI7IN42D66t"
      },
      "source": [
        "####**Replace null values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzGzb3S-DajE"
      },
      "source": [
        "# Checking for missing values in the dataset\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJBQuY5UEApB"
      },
      "source": [
        "# dropping the nan values\n",
        "df = df.fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwtVu1KC7-WD"
      },
      "source": [
        "# Now count the Unique values to check the data is balanced or not\n",
        "count = np.unique(df['label'], return_counts=True)\n",
        "count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2yfo4JlTIyd"
      },
      "source": [
        "Now let's see that graphically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DryA8rqf7-YV"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x='label', data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8_tu43nTeRj"
      },
      "source": [
        "####**Extra EDA**\n",
        "\n",
        "Let's check the 10 authors who created the most articles with fake news and compare them to the top 20 authors who created accurate articles (not fake)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEljdykDaVTa"
      },
      "source": [
        "df.loc[df['label'] == 1]['author'].value_counts()[:10].sort_values().plot(kind = 'barh',figsize=(18,6),title=\"Top 10 Authors Making Fake News/Articles\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln1GGEagbqxm"
      },
      "source": [
        "df.loc[df['label'] == 0]['author'].value_counts()[:10].sort_values().plot(kind = 'barh',figsize=(18,6),title=\"Top 10 Authors Making Factually Correct Articles\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktpFtc8vEFT-"
      },
      "source": [
        "###**Preprocessing Text**\n",
        "\n",
        "We will perform the below preprocessing tasks:\n",
        "\n",
        "- Convert everything to lowercase - We don't want our model to think having capitals and non capitals in text are significant indications of an article being fake or not... So let's set everything to lower case.\n",
        "- Remove HTML tags\n",
        "- Remove URLs from sentences\n",
        "- Contraction mapping - This essentially fixes and expands shortened words - you're -> you are\n",
        "- Eliminate punctuations and special characters\n",
        "- Remove stopwords - words that are usually irrelevant to the meaning of a piece of text\n",
        "- Stem words in text - This essentially removes the suffix of a word and gives back the base/root of a word (e.g. Flying becomes Fly)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjV70TkaEuJL"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtVrNUP_8K4b"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-_U2yEkEoPI"
      },
      "source": [
        "stopword_list = stopwords.words('english')\n",
        "print(stopword_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2poYz0PBE9f7"
      },
      "source": [
        "# let us use the contractions package to fix and expand shortened words - you're -> you are\n",
        "import contractions\n",
        "def decontracted(sentance):\n",
        "    expanded_words = []    \n",
        "    for word in sentance.split():\n",
        "      expanded_words.append(contractions.fix(word))   \n",
        "\n",
        "    expanded_text = ' '.join(expanded_words)\n",
        "    return expanded_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoGibiQ0E9jW"
      },
      "source": [
        "def sentence_clean(sentence):\n",
        "    # change sentence to lower case\n",
        "    sentence = sentence.lower()\n",
        "    # removing URL from sentence\n",
        "    sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
        "    # removing HTML tags\n",
        "    sentence = BeautifulSoup(sentence, 'lxml').get_text()\n",
        "    # removing contraction of words from sentence   # call decontracted funtion for it\n",
        "    sentence = decontracted(sentence)\n",
        "    # removing digits\n",
        "    sentence = re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
        "    # removing special character\n",
        "    sentence = re.sub('[^A-Za-z]+', ' ', sentence)\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pftasaVE9mI"
      },
      "source": [
        "# Use Stemming \n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Performing the preprocessing steps on all messages\n",
        "def preprocess(document):\n",
        "    preprocessed_reviews = []\n",
        "    # tqdm is for printing the status bar\n",
        "    for sentence in tqdm(document):\n",
        "        # call sentence_clean function to clean text\n",
        "        sentence = sentence_clean(sentence)\n",
        "        # tokenize into words\n",
        "        words = word_tokenize(sentence)\n",
        "        # remove stop words\n",
        "        tokens = [ps.stem(word) for word in words if word not in stopword_list]\n",
        "\n",
        "        # join words to make sentence\n",
        "        sentence = \" \".join(tokens).strip()\n",
        "\n",
        "        preprocessed_reviews.append(sentence)\n",
        "        \n",
        "    return preprocessed_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBy9ePzJFLkx"
      },
      "source": [
        "%%time\n",
        "corpus = preprocess(df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_AZNmy2GPI4"
      },
      "source": [
        "#### **Try to spot the differences in the text!**\n",
        "\n",
        "What has stemming the words done?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6VGpV16FLnm"
      },
      "source": [
        "print(\"Before preprocess\\n\", df['text'][1])\n",
        "print(\"***\"*40)\n",
        "print(\"After preprocess\\n\", corpus[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhRzqThEFLpl"
      },
      "source": [
        "df['text'] = corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUGtmhjDCrJJ"
      },
      "source": [
        "## **3. Build Our Training And Test Data**\n",
        "<body>\n",
        "<img src=\"https://clearmeasure.com/wp-content/uploads/2018/11/build-1159776_960_720.jpg\"/>\n",
        "</body>\n",
        "\n",
        "**In this section we are seperating our data between our X and y.**\n",
        "\n",
        "- X will be the data the model recieves and in turn makes a prediction.\n",
        "\n",
        "- y will be the data the model will compare its prediction to i.e. the data the model marks itself against\n",
        "\n",
        "**Then the data will be split into training data and test data.**\n",
        "\n",
        "Depending on how much data you have (the more the better), we split it into our training & test data. This means we create a training data to test data ratio between 0.7 to 0.3 and 0.8 to 0.2.\n",
        "\n",
        "We want to give our model plenty of data to learn from but we also need to give it enough unseen tests for it to accurately gauge its predictive ability.\n",
        "\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary>Parameters you can change:</summary>\n",
        "\n",
        "  - test_size - As stated before, this parameter will determine your training data to test data ratio.\n",
        "\n",
        "    -  0.5 means that we are creating a 50/50 split between training and test data\n",
        "    - 0.25 means that we are creating a 75/25 split between training and test data\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI18xioG8K7L"
      },
      "source": [
        "# Seperating the data and the label \n",
        "X = df['text']\n",
        "y = df['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE-qXNb_HMs0"
      },
      "source": [
        "# split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orfZOkLWHMvX"
      },
      "source": [
        "print(\"Train size:\", X_train.shape)\n",
        "print(\"Test size:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uelIYEytHv77"
      },
      "source": [
        "##**4. Create our Model**\n",
        "<body>\n",
        "<img src=\"https://scx2.b-cdn.net/gfx/news/hires/2019/howtoovercom.jpg\" width=\"870\"/>\n",
        "</body>\n",
        "\n",
        "For our fake news detection model, we will be using a logistic regression model.\n",
        "\n",
        "In simple terms,Logistic Regression is a statistical model used to model the probability of an event existing i.e. what is the probability that you will win or loose (in this example an article being real or fake)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFrGD2ZWHMyu"
      },
      "source": [
        "def plot_confusion_matrix(y_actual, y_pred):\n",
        "    '''\n",
        "    This method plots confusion matrix.\n",
        "    '''\n",
        "    classes = ['Fake News', 'Real News']\n",
        "    tick_marks = np.arange(len(classes))\n",
        "\n",
        "    accuracy = accuracy_score(y_actual, y_pred)\n",
        "    print(\"Accuracy score:\", \"{:2.3}\".format(accuracy))\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_actual, y_pred)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4, 4))\n",
        "    ax.matshow(conf_matrix, cmap=plt.cm.Reds, alpha=0.3)\n",
        "    for i in range(conf_matrix.shape[0]):\n",
        "        for j in range(conf_matrix.shape[1]):\n",
        "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.xticks(tick_marks , classes, rotation=0)\n",
        "    plt.yticks(tick_marks , classes)\n",
        "    plt.xlabel('Predictions')\n",
        "    plt.ylabel('Actuals')\n",
        "    plt.title('Confusion Matrix', fontsize=12)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7hRfPi3IGwX"
      },
      "source": [
        "### **TF-IDF Vectorizing**\n",
        "<body>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*qQgnyPLDIkUmeZKN2_ZWbQ.png\" width=\"870\"/>\n",
        "</body>\n",
        "\n",
        "\n",
        "In similar terms as the image says above, **TF-IDF** (**Term Frequency- Inverse Document Frequency**), is a numerical statistic reflecting how important a word is to a document in a collection. We can use this to pick out words that appear to be important in our test article. These will be used as our features - words that the model will use to learn if an article is fake or not.\n",
        "\n",
        "\n",
        "**Don't worry you don't have to know any formulas for this!!**\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary>Parameters you can change:</summary>\n",
        "\n",
        "  - n_gram - This parameter picks out our features in ranges from 1 to n (1,n). \n",
        "\n",
        "    -  (1,1) means that only one word features will be extracted and used as features.\n",
        "    -  (1,2) means that one word and two word terms will be extracted and used as features.\n",
        "    -  (1,3) means that one word, two word and three word terms will be extracted and used as features.\n",
        "\n",
        "**Example**\n",
        "\n",
        "  Text = \"I am writing this text as an example to show you the importance of tf-idf vectorizers\"\n",
        "  1. n_gram(1,1) might pick out words like [\"writing\",\"example\"...] etc\n",
        "\n",
        "  2. n_gram(1,2) might pick out words like [\"writing\", \"writing this\",\"example\"...] etc\n",
        "\n",
        "  3. n_gram(1,3) might pick out words like [\"importance\", importance of\", importance of tf-idf\"...] etc\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YuhvC-wc-fF"
      },
      "source": [
        "#test out n_gram range\n",
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
        "tfidf.fit(X) # adjust our vectorizer to the data we have\n",
        "\n",
        "#Now convert the data into their tfidf representations\n",
        "X_train_cv = tfidf.transform(X_train)\n",
        "X_test_cv = tfidf.transform(X_test)\n",
        "\n",
        "len(tfidf.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxQwamvXHM26"
      },
      "source": [
        "# Create our logistic regression object/model object and fit it to our training data\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_cv, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHokE_9iHM5C"
      },
      "source": [
        "#predict on training data\n",
        "X_train_predict = model.predict(X_train_cv)\n",
        "#Let's see our training accuracy\n",
        "train_accuracy = accuracy_score(y_train, X_train_predict)\n",
        "\n",
        "#predict on our test data\n",
        "X_test_predict = model.predict(X_test_cv)\n",
        "#Let's see our test accuracy\n",
        "test_accuracy = accuracy_score(y_test, X_test_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0y4Pf9lHM7v"
      },
      "source": [
        "accuracy = accuracy_score(y_test, X_test_predict)\n",
        "LR_TF_TFIDF = {'Vectorizer': 'TF-IDF', 'Algorithm': 'Logistic_Regression_1', \n",
        "               'Train Accuracy':train_accuracy, 'Test Accuracy':test_accuracy}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90yghsbnZ3Mu"
      },
      "source": [
        "LR_TF_TFIDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1SMqa1XakUu"
      },
      "source": [
        "Understanding our model's performance with a confusion matrix helps us to see exactly how our model performed.\n",
        "A confusion helps to identify where the model made false positives and false negatives compared to the correct answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jF6OOGgHM9x"
      },
      "source": [
        "# plot confusion matrix on test\n",
        "plot_confusion_matrix(y_test, X_test_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZWg7nHCu4c0"
      },
      "source": [
        "## **5. Test our model**\n",
        "\n",
        "Let us test our model with our own examples and some examples from the training and test data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZJd5ZpPZ698"
      },
      "source": [
        "def classify_message(text):\n",
        "    text = tfidf.transform(text)\n",
        "    predicted = model.predict(text)\n",
        "    probability = model.predict_proba(text).max()*100\n",
        "\n",
        "    if predicted==0:\n",
        "      print(\" I am \"+ str(round(probability))+\"% sure that this is not Fake news\")\n",
        "    else:\n",
        "      print(\" I am \"+ str(round(probability))+\"% sure that this news is Fake\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upy3cuc-gT_Z"
      },
      "source": [
        "test2 = [\"Share a certain post of Bill Gates on Facebook and he will send you money.\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18BQ25vvgax0"
      },
      "source": [
        "classify_message(test2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcD77847aA_d"
      },
      "source": [
        "df[['title','text','label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_nz06JKaIwy"
      },
      "source": [
        "classify_message([df['text'][1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7oFIyJreL08"
      },
      "source": [
        "###**Apply it to our test data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwlLl20bfGl2"
      },
      "source": [
        "def classify_message(text):\n",
        "    text = tfidf.transform(text)\n",
        "    predicted = model.predict(text)\n",
        "    probability = model.predict_proba(text).max()*100\n",
        "\n",
        "    if predicted==0:\n",
        "      return(\"Not Fake\")\n",
        "    else:\n",
        "      return(\"Fake\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DOZQaWhfYFm"
      },
      "source": [
        "def prediciton_score(text):\n",
        "    text = tfidf.transform(text)\n",
        "    predicted = model.predict(text)\n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXJwQav7eWYY"
      },
      "source": [
        "#create a new column called prediction with no data\n",
        "test_data['prediction'] = \" \"\n",
        "#create a new column called prediction_score with no data\n",
        "test_data['prediction_score'] = \" \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Fw7LXDf2TA"
      },
      "source": [
        "test_data = test_data.fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mhKy2eBePzG"
      },
      "source": [
        "#create a for loop which takes in data from the text column and returns a prediction as to whether the news is fake or not\n",
        "for i in range (len(test_data)):\n",
        "  test_data['prediction'][i] = classify_message([test_data['text'][i]])\n",
        "  test_data['prediction_score'][i] = prediciton_score([test_data['text'][i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXD4HJlXf4Ku"
      },
      "source": [
        "test_data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}